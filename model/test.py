import csv
import re
import pickle
import numpy as np
import tensorflow as tf

class Predictor:
    """Class for predicting the log events generated by Docker.
    """
    def __init__(self) -> None:
        """Initializes the object of the Predictor class.
        """
        self.label_file_path = r'Dataset\test\test_label.csv' # Path to labels file.
        self.template_embeddings_file_path = r'Dataset\test\template_embs.pickle' # Path to the template embeddings file.
        # Reading the template embeddings.
        with open(self.template_embeddings_file_path, 'rb') as file:
            self.template_embs = pickle.load(file)
        self.vocab_size = len(self.template_embs[0]) # Vocab size is the dimension of each template embedding.
        self.label2id = {"Normal" : 0,
                         "Anomaly" : 1} # A dictionary maping of label to id.
        self.model_file_path = r'model.keras' # Path to the trained ML model.
        self.model = tf.keras.models.load_model(self.model_file_path) 
        self.blockid2label = {} # Dictionary maping of blockid to label.
        self.blockid_pattern = r'blk_\-?[0-9]*' # Regex pattern to match for block id.
        self.templates = [r'.*?Receiving block.*?src:.*?dest:.*',
                r'.*?Received block.*?of size.*?from.*',
                r'.*?PacketResponder.*?for block.*?terminating.*',
                r'.*?BLOCK\* NameSystem.*?addStoredBlock: blockMap updated:.*?is added to.*?size.*',
                r'.*?BLOCK\* NameSystem.*?allocateBlock:.*',
                r'.*?Verification succeeded for.*',
                r'.*?Adding an already existing block.*',
                r'.*?Served block.*?to.*',
                r'.*?Got exception while serving.*?to.*',
                r'.*?Received block.*?src:.*?dest:.*?of size.*',
                r'.*?writeBlock.*?received exception.*',
                r'.*?PacketResponder.*?for block.*?Interrupted.*',
                r'.*?PacketResponder.*?Exception.*',
                r'.*?:Exception writing block.*?to mirror.*',
                r'.*?Receiving empty packet for block.*',
                r'.*?Exception in receiveBlock for block.*',
                r'.*?Changing block file offset of block.*?from.*?to.*?meta file offset to.*',
                r'.*?:Transmitted block.*?to.*',
                r'.*?:Failed to transfer.*?to.*?got.*',
                r'.*?Starting thread to transfer block.*?to.*',
                r'.*?Reopen Block.*',
                r'.*?Unexpected error trying to delete block.*?BlockInfo not found in volumeMap.*',
                r'.*?Deleting block.*?file.*',
                r'.*?BLOCK\* NameSystem.*?delete:.*?is added to invalidSet of.*',
                r'.*?BLOCK\* Removing block.*?from neededReplications as it does not belong to any file.*',
                r'.*?BLOCK\* ask.*?to replicate.*?to.*',
                r'.*?BLOCK\* NameSystem.*?addStoredBlock: Redundant addStoredBlock request received for.*?on.*?size.*',
                r'.*?BLOCK\* NameSystem.*?addStoredBlock: addStoredBlock request received for.*?on.*?size.*?But it does not belong to any file.*',
                r'.*?PendingReplicationMonitor timed out block.*']
      
    def read_labels(self) -> None:
        """Reads the labels file and creates a dictionary maping of blockid to label.
        """
        # Loads the label file and creates a dictionary mapping of block id to label.
        with open(self.label_file_path, 'r') as file:
            csv_reader = csv.reader(file) # Reads the csv file.
            for row in csv_reader:
                # In the CSV file, each row represents one block. 
                # First column gives the block id.
                # Second gives the label.
                block_id = row[0]
                label = row[1]
                self.blockid2label[block_id] = label

        # The first row in the csv file is the headers, which we have to remove.
        del(self.blockid2label['BlockId'])

    def create_template_seq (self, log_lines: list) -> dict:
        """Creates a dictionary maping of blockid to a list of template ids.

        Args:
            log_lines (list): A list of log lines.

        Returns:
            dict: A dictionary maping of blockid to a list of template ids.
        """
        blockid2temp_seq = {} # A dictionary maping of blockid to a list of template ids.

        for line in log_lines:
            line = line.strip()
            # Looping through each of the templates.
            for i, template in enumerate(self.templates):
                # Searching for the template in the current line.
                if re.search(template, line):
                    # Searching for the block id in the line.
                    blockid = re.search(self.blockid_pattern, line).group()
                    # Updates the blockid2temp_seq dictionary.
                    if blockid in blockid2temp_seq:
                        blockid2temp_seq[blockid].append(i)
                    else:
                        blockid2temp_seq[blockid] = [i]
                    break
        
        return blockid2temp_seq

    def preprocess(self,log_lines: list) -> tuple:
        """Preprocess the log lines to create dataset_X and dataset_Y for model prediction.

        Args:
            log_lines (list): A list of log lines.
        Returns:
            tuple: (datset_X, dataset_Y)
        """
        #blockid2label = read_labels(label_file_path) # Gets a dictionary maping of blockid2labels.
        blockid2temp_seq = self.create_template_seq(log_lines) # Gets a dictionary maping of blockid to a list of templates ids.

        dataset_X = [] # The list for vector representations of the inputs.
        #dataset_Y = [] # The list for labels.
        
        # Looping through the blockids in the list.
        for blockid in blockid2temp_seq:
            # Appeding the label to the list.
            #dataset_Y.append(label2id[blockid2label[blockid]])
            vector_rep = np.zeros(self.vocab_size) # Initializing the representation with all zeroes.
            # Looping through each template id in the sequence.
            for template_id in blockid2temp_seq[blockid]:
                vector_rep += self.template_embs[template_id] # Adds up the vector representaion for each template.
            
            dataset_X.append(vector_rep) # Appends the representation to the list. 
        dataset_X = np.array(dataset_X) # Converts to an NP array.
        return dataset_X#, dataset_Y

    def predict(self,log_lines: list) -> tuple:
        """Predicts the number of normal and anomalous blocks from the given list of log lines.

        Args:
            log_lines (list): A list of log lines.

        Returns:
            tuple: (num_normal, num_anomaly)
        """
        
        # Preprocessing the log lines to generate datset_X and datset_Y.
        dataset_X = self.preprocess(log_lines)
    
        # Making predictions.
        Y_pred_probs = self.model.predict(dataset_X)  # Getting probabilities for each class.
        Y_pred = (Y_pred_probs > 0.5).astype(int) # Converting probability to class labels.

        # Counting normal and anomaly.
        num_normal = np.sum(Y_pred == 0)
        num_anomaly = np.sum(Y_pred == 1)

        return num_normal, num_anomaly  
